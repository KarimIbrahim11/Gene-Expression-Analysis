{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation \n",
    "#### This notebook is used to transform the data from it's Original Format to a one more suited for the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries and setting up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.data import *\n",
    "from src.utils.memory_management import *\n",
    "from src.configs.config_parser import PathConfigParser, data_config_file, project_root\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a console handler\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Configs from Config Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs Directory\n",
    "parser = PathConfigParser(str(data_config_file))\n",
    "parser.load()\n",
    "\n",
    "# Access paths\n",
    "RAW_DATA_PATH = project_root / parser.get(\"data_paths\", {}).get(\"raw_data\") \n",
    "PROCESSED_DATA_PATH = project_root / parser.get(\"data_paths\", {}).get(\"processed_data\")\n",
    "GE_PATH = parser.get(\"data_paths\", {}).get(\"brain_regions_genes_ge\")\n",
    "PROCESSED_DONORS_GE_PATH = PROCESSED_DATA_PATH / GE_PATH\n",
    "\n",
    "# Donors_ids\n",
    "DONORS_IDS = parser.get(\"donors_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(project_root/ Path(\"data/download_dataset.py\")) as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions for the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sample_annotations(donor_sa: pd.DataFrame, left_mask : pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Transforming Sample Annotations initial files\n",
    "    \"\"\"\n",
    "    # Keep certain columns in the SampleAnnot.csv file\n",
    "    donor_sa_processed = keep_df_cols(donor_sa, [\"structure_id\", \"structure_name\", \"well_id\", \"mri_voxel_x\", \"mri_voxel_y\", \"mri_voxel_z\",\t\"mni_x\", \"mni_y\", \"mni_z\"])\n",
    "    deallocate_df(donor_sa)\n",
    "    # Applying mask on Sample annotations file\n",
    "    donor_sa_filtered= donor_sa_processed[left_mask].reset_index(drop=True)\n",
    "    deallocate_df(donor_sa_processed)\n",
    "    return donor_sa_filtered\n",
    "\n",
    "def transform_gene_expressions(donor_ge: pd.DataFrame, left_mask : pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Transforming Gene Expression initial files\n",
    "    \"\"\"\n",
    "    # Applying mask on the Columns of the donor_ge file\n",
    "    left_mask_ge = pd.concat([pd.Series([True], index=[0]), left_mask], ignore_index=True)\n",
    "    left_mask_ge_filtered = left_mask_ge[left_mask_ge].index.values.tolist()\n",
    "    donor_ge_filtered  = donor_ge.iloc[:, left_mask_ge_filtered]\n",
    "    deallocate_df(donor_ge)\n",
    "    return donor_ge_filtered\n",
    "\n",
    "def write_geneexpressions_to_json(df: pd.DataFrame, pth: Path) -> None:\n",
    "    \"\"\"\n",
    "        Specific to writing gene_expression files to json and keep the lists as numbers\n",
    "    \"\"\"\n",
    "    # Ensure 'gene_expression_values' is a list, not a string\n",
    "    df[\"gene_expression_values\"] = df[\"gene_expression_values\"].apply(\n",
    "        lambda x: literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    # Group by `brain_region`\n",
    "    grouped = (\n",
    "        df.groupby(\"brain_region\")\n",
    "        .apply(lambda x: x[[\"gene_id\", \"gene_expression_values\"]].to_dict(orient=\"records\"))\n",
    "        .to_dict()\n",
    "    )\n",
    "    # Change group keys to number\n",
    "    grouped = {int(key): value for key, value in grouped.items()}\n",
    "\n",
    "    # Write grouped data to JSON\n",
    "    with open(pth, \"w\") as f:\n",
    "        json.dump(grouped, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Grouped CSV Files for each Donor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a regex to load the files based upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_pattern = r\"^normalized_microarray_donor\\d+$\"\n",
    "donor_dirs = [d for d in RAW_DATA_PATH.iterdir() if d.is_dir() and re.match(donor_pattern, d.name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the files and processing them individually in `data/processed/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for donor_path in donor_dirs:\n",
    "    # Processing path\n",
    "    logger.info(f\"Processing data of {donor_path}\")\n",
    "\n",
    "    # Processing SampleAnnot to get the brain region id (\"structure id\")\n",
    "    donor_sa = load_df_from_csv(donor_path / \"SampleAnnot.csv\")\n",
    "    # Create a mask for left hemisphere entries\n",
    "    left_mask = mask_left_hemisphere(donor_sa)\n",
    "    # Transform donor_sa\n",
    "    donor_sa_filtered = transform_sample_annotations(donor_sa, left_mask)\n",
    "\n",
    "    # Loading Gene Expressions\n",
    "    donor_ge = load_df_from_csv(donor_path / \"MicroarrayExpression.csv\")\n",
    "    # Transform Gene Expressions\n",
    "    donor_ge_filtered = transform_gene_expressions(donor_ge, left_mask)\n",
    "    # Load probes data \n",
    "    donor_probes = load_df_from_csv(donor_path / \"Probes.csv\")\n",
    "    # Add Brain Region id as a column name in the gene_expression data\n",
    "    donor_ge_filtered.columns = ['probe_id'] + list(donor_sa_filtered[\"structure_id\"])\n",
    "    deallocate_df(donor_sa_filtered)\n",
    "    # Add gene_id Column in the gene_expressions data\n",
    "    donor_ge_filtered.insert(0, 'gene_id', donor_probes['gene_id'])\n",
    "\n",
    "    # Drop probe_id column        \n",
    "    donor_ge_filtered = donor_ge_filtered.drop(columns=['probe_id'])\n",
    "    \n",
    "    # Create a new CSV with grouped gene_ids by melting the DataFrame to make it easier to manipulate\n",
    "    df_melted = donor_ge_filtered.melt(id_vars=[\"gene_id\"], var_name=\"brain_region\", value_name=\"gene_expression_values\")\n",
    "    # Now, group by brain_region and gene_id  and aggregate the test values into lists\n",
    "    df_grouped = df_melted.groupby([\"brain_region\", \"gene_id\"])[\"gene_expression_values\"].apply(list).reset_index()\n",
    "    # *Apply json dumps to be able to load the file appropiately\n",
    "    df_grouped[\"gene_expression_values\"] = df_grouped[\"gene_expression_values\"].apply(json.dumps)\n",
    "    # Save the grouped csvs per each donor\n",
    "    write_df_to_csv(df_grouped, PROCESSED_DATA_PATH / f\"brain_regions_genes_geneexpressions/{get_donor_id_from_path(donor_path)}_grouped.csv\")\n",
    "    deallocate_df(df_melted)\n",
    "    deallocate_df(donor_ge_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for donor_path in donor_dirs:\n",
    "    # Processing path\n",
    "    logger.info(f\"Processing data of {donor_path}\")\n",
    "\n",
    "    # Processing SampleAnnot to get the brain region id (\"structure id\")\n",
    "    donor_sa = load_df_from_csv(donor_path / \"SampleAnnot.csv\")\n",
    "    # Create a mask for left hemisphere entries\n",
    "    left_mask = mask_left_hemisphere(donor_sa)\n",
    "    # Transform donor_sa\n",
    "    donor_sa_filtered = transform_sample_annotations(donor_sa, left_mask)\n",
    "\n",
    "    # Loading Gene Expressions\n",
    "    donor_ge = load_df_from_csv(donor_path / \"MicroarrayExpression.csv\")\n",
    "    # Transform Gene Expressions\n",
    "    donor_ge_filtered = transform_gene_expressions(donor_ge, left_mask)\n",
    "    # Load probes data \n",
    "    donor_probes = load_df_from_csv(donor_path / \"Probes.csv\")\n",
    "    # Add Brain Region id as a column name in the gene_expression data\n",
    "    donor_ge_filtered.columns = ['probe_id'] + list(donor_sa_filtered[\"structure_id\"])\n",
    "    \n",
    "        \n",
    "    # donor_ge_filtered.columns = donor_ge_filtered.columns + list(donor_sa_filtered[\"mri_voxel_x\"])\n",
    "    \n",
    "    deallocate_df(donor_sa_filtered)\n",
    "    # Add gene_id Column in the gene_expressions data\n",
    "    donor_ge_filtered.insert(0, 'gene_id', donor_probes['gene_id'])\n",
    "\n",
    "    # Drop probe_id column        \n",
    "    donor_ge_filtered = donor_ge_filtered.drop(columns=['probe_id'])\n",
    "    \n",
    "    \n",
    "    # Add additional columns from donor_sa_filtered as a new row\n",
    "    # Extract the additional columns\n",
    "    additional_columns = donor_sa_filtered[[\"mri_voxel_x\", \"mri_voxel_y\", \"mri_voxel_z\", \"mni_x\", \"mni_y\", \"mni_z\"]]\n",
    "    \n",
    "    # Transpose the additional columns to make them a row\n",
    "    additional_row = additional_columns.T  # Transpose to make columns into a row\n",
    "    additional_row.columns = donor_ge_filtered.columns  # Align columns with donor_ge_filtered\n",
    "    \n",
    "    # Append the additional row to donor_ge_filtered\n",
    "    donor_ge_filtered = pd.concat([donor_ge_filtered, additional_row], ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Create a new CSV with grouped gene_ids by melting the DataFrame to make it easier to manipulate\n",
    "    df_melted = donor_ge_filtered.melt(id_vars=[\"gene_id\"], var_name=\"brain_region\", value_name=\"gene_expression_values\")\n",
    "    # Now, group by brain_region and gene_id  and aggregate the test values into lists\n",
    "    df_grouped = df_melted.groupby([\"brain_region\", \"gene_id\"])[\"gene_expression_values\"].apply(list).reset_index()\n",
    "    # *Apply json dumps to be able to load the file appropiately\n",
    "    df_grouped[\"gene_expression_values\"] = df_grouped[\"gene_expression_values\"].apply(json.dumps)\n",
    "    # Save the grouped csvs per each donor\n",
    "    write_df_to_csv(df_grouped, PROCESSED_DATA_PATH / f\"brain_regions_genes_geneexpressions/{get_donor_id_from_path(donor_path)}_grouped.csv\")\n",
    "    deallocate_df(df_melted)\n",
    "    deallocate_df(donor_ge_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Donor CSV Creation and Selecting only Common brain regions for the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating `meta_donor.csv` which is the file having the combined data from all donors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_ges = []\n",
    "for donor in DONORS_IDS:\n",
    "    # Load donor .csv from processed data \n",
    "    donor_ge = load_df_from_csv(PROCESSED_DONORS_GE_PATH / Path(f\"{donor}_grouped.csv\"))\n",
    "    logger.info(f\"Donor Id: {str(donor)}\")\n",
    "    logger.info(f\"Number of brain regions: {donor_ge['brain_region'].nunique()}\")\n",
    "    logger.info(f\"Number of gene ids: {donor_ge['gene_id'].nunique()}\")\n",
    "    donor_ge[\"gene_expression_values\"]=donor_ge[\"gene_expression_values\"].apply(json.loads)\n",
    "    donor_ges.append(donor_ge)\n",
    "    deallocate_df(donor_ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brain_region = set.union(*(set(donor_ge['brain_region']) for donor_ge in donor_ges))\n",
    "len(all_brain_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding common brain regions and filtering the others out\n",
    "common_brain_regions = set.intersection(*(set(donor_ge['brain_region']) for donor_ge in donor_ges))\n",
    "logger.info(f\"Number of common brain regions: {len(common_brain_regions)}\")\n",
    "\n",
    "# Keep only the common columns in each DataFrame\n",
    "filtered_donors_ges = [donor_ge[donor_ge['brain_region'].isin(common_brain_regions)] for donor_ge in donor_ges]\n",
    "set(filtered_donors_ges[3]['brain_region'])==common_brain_regions\n",
    "\n",
    "# Concatenate all filtered DataFrames\n",
    "meta_donor_df = pd.concat(filtered_donors_ges, ignore_index=True)\n",
    "logger.info(f\"meta_donor_df size:{len(meta_donor_df)}\")\n",
    "logger.info(f\"Meta Donor DF has only list of common_brain_regions: {set(meta_donor_df['brain_region'])==common_brain_regions}\")\n",
    "\n",
    "concatenated_ges = meta_donor_df.groupby([\"brain_region\", \"gene_id\"])[\"gene_expression_values\"].apply(lambda x: sum(x, [])).reset_index()\n",
    "\n",
    "write_df_to_csv(concatenated_ges, PROCESSED_DONORS_GE_PATH / f\"meta_donor.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Transforming files to .json for hierarchal structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating .json files for Hierarchal Data\n",
    "for donor in DONORS_IDS:\n",
    "    logger.info(f\"Creating donor id: {str(donor)} .json file\")\n",
    "    # Load donor .csv from processed data \n",
    "    donor_ge = load_df_from_csv(PROCESSED_DONORS_GE_PATH / Path(f\"{donor}_grouped.csv\"))\n",
    "    write_geneexpressions_to_json(donor_ge, PROCESSED_DONORS_GE_PATH / Path(f\"{donor}_grouped.json\"))\n",
    "# Creating Meta Donor Json File\n",
    "logger.info(f\"Creating meta_donor.json file\")\n",
    "meta_donor_ge = load_df_from_csv(PROCESSED_DONORS_GE_PATH / Path(f\"meta_donor.csv\"))\n",
    "write_geneexpressions_to_json(meta_donor_ge, PROCESSED_DONORS_GE_PATH / Path(f\"meta_donor.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-Ccial1dq-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
